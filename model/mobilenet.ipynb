{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86422,"status":"ok","timestamp":1709309803847,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"},"user_tz":-420},"id":"yT4qAsdyF4pg","outputId":"33dbec17-c4c5-40d1-f12b-67682af10378"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"fzmkEoxBHQ4W"},"source":["#Import các thư viện cần thiết"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFP5caF6HUmg"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose,Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam,RMSprop\n","from tensorflow.keras import backend as K\n","import sklearn\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"0K0df0eS4hdW"},"source":["#Create labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfK_kVFJ2vfz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709309817746,"user_tz":-420,"elapsed":539,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"}},"outputId":"32d2f93c-3a7e-43af-f930-33ac5704e35c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-01 16:16:57--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2024-03-01 16:16:57 (88.7 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}],"source":["# System libraries\n","from pathlib import Path\n","import os.path\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","\n","# Import series of helper functions for our notebook\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-eWB_5V3GLW"},"outputs":[],"source":["# Walk through each directory\n","dataset = '/content/drive/MyDrive/Colab Notebooks/data_100'\n","\n","walk_through_dir(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrrFbbTK3jJ8"},"outputs":[],"source":["import pandas as pd\n","image_dir = Path(dataset)\n","\n","# Get filepaths and labels\n","filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png')) + list(image_dir.glob(r'**/*.jpeg'))\n","\n","labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n","\n","filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n","labels = pd.Series(labels, name='Label')\n","\n","# Concatenate filepaths and labels\n","image_df = pd.concat([filepaths, labels], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2011,"status":"ok","timestamp":1708697269428,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"},"user_tz":-420},"id":"ys0G-qLP4Klj","outputId":"5d41910e-52ec-4603-973f-e8f5b7fd064f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["42196"]},"metadata":{},"execution_count":6}],"source":["len(list(image_dir.glob(r'**/*.jpeg')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUAb_MtP4OTi"},"outputs":[],"source":["train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"LDZYRJJi5RDO"},"source":["# data aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VlsRzw95v7c"},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n","train_generator = ImageDataGenerator(\n","    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n","    validation_split=0.2\n",")\n","test_generator = ImageDataGenerator(\n","    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14605,"status":"ok","timestamp":1708697292969,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"},"user_tz":-420},"id":"7mlHObIX53oT","outputId":"5566bfaf-1da9-458e-da57-2a72e527f422"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 27600 validated image filenames belonging to 530 classes.\n","Found 6899 validated image filenames belonging to 530 classes.\n","Found 8625 validated image filenames belonging to 530 classes.\n"]}],"source":["# Split the data into three categories.\n","train_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=(128, 128),\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=42,\n","    subset='training'\n",")\n","\n","val_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=(128, 128),\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=42,\n","    subset='validation'\n",")\n","\n","test_images = test_generator.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=(128, 128),\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=32,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYAt6QX-8NkW"},"outputs":[],"source":["# Resize Layer\n","from tensorflow.keras import layers,models\n","resize_and_rescale = tf.keras.Sequential([\n","  layers.experimental.preprocessing.Resizing(128,128),\n","  layers.experimental.preprocessing.Rescaling(1./255),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2489,"status":"ok","timestamp":1708697323845,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"},"user_tz":-420},"id":"fGi02o-P8fVo","outputId":"23f4b1f9-e098-46b4-a183-bf2f808a9b91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n","9406464/9406464 [==============================] - 1s 0us/step\n"]}],"source":["# Load the pretained model\n","pretrained_model = tf.keras.applications.MobileNetV2(\n","    input_shape=(128, 128, 3),\n","    include_top=False,\n","    weights='imagenet',\n","    pooling='avg'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OT2OZ1uClR1f"},"outputs":[],"source":["pretrained_model.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8k_p6nR8j_A"},"outputs":[],"source":["# Create checkpoint callback\n","checkpoint_path = \"images_classification_model_checkpoint\"\n","checkpoint_callback = ModelCheckpoint(checkpoint_path,\n","                                      save_weights_only=True,\n","                                      monitor=\"val_accuracy\",\n","                                      save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvypMpKX8o_d"},"outputs":[],"source":["# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n","from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n","\n","early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n","                               patience = 5,\n","                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training"]},{"cell_type":"code","source":["# Hủy đóng băng mô hình cơ sở\n","pretrained_model.trainable = True"],"metadata":{"id":"7MVSvP69P1pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3XZLPQE8y5P","outputId":"01dd9697-49e5-4dbb-ae65-096619b90f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: training_logs/images_classification/20240223-140853\n","Epoch 1/20\n"," 37/863 [>.............................] - ETA: 6:04:37 - loss: 6.4665 - accuracy: 0.0017"]}],"source":["# Tensorflow Libraries\n","from tensorflow import keras\n","from tensorflow.keras import layers,models\n","from keras.layers import Dense, Dropout, Softmax\n","from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","inputs = pretrained_model.input\n","x = resize_and_rescale(inputs)\n","x = Dense(1024, activation='sigmoid')(pretrained_model.output)\n","x = Dropout(0.2)(x)\n","x = Dense(512, activation='sigmoid')(x)\n","x = Dropout(0.2)(x)\n","outputs = Dense(530, activation='softmax')(x)\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(\n","    optimizer=Adam(), #0.0001\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=20,\n","    callbacks=[\n","        create_tensorboard_callback(\"training_logs\",\n","                                    \"images_classification\"),\n","        checkpoint_callback,early_stopping\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4KA8mxt9pFi"},"outputs":[],"source":["results = model.evaluate(test_images, verbose=0)\n","\n","print(\"    Test Loss: {:.5f}\".format(results[0]))\n","print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n"]},{"cell_type":"code","source":["model.save('mobilenet.h5')\n"],"metadata":{"id":"T1PpqwGnQkeH","executionInfo":{"status":"ok","timestamp":1707069313369,"user_tz":-420,"elapsed":958,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"}},"outputId":"9a3053c7-98dc-4eb1-a47a-91d19ad48c1c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"markdown","metadata":{"id":"PI1T20Q9jT0L"},"source":["# MobileNetV2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcXfgj0YmOxy"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, Softmax\n","from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.optimizers import Adam\n","\n","class MobileNetV1(tf.keras.Model):\n","    def __init__(self, input_shape=(128, 128, 3), num_classes=530):\n","        super(MobileNetV1, self).__init__()\n","\n","        self.mobilenet_base = tf.keras.applications.MobileNetV2(\n","            input_shape=input_shape,\n","            include_top=False,\n","            weights='imagenet',\n","            pooling='avg'\n","        )\n","        self.flatten = Flatten()\n","        self.dense_1 = Dense(1024, activation='sigmoid')\n","        self.dropout_1 = Dropout(0.2)\n","        self.dense_2 = Dense(256, activation='sigmoid')  # Changed the name to dense_2\n","        self.dropout_2 = Dropout(0.2)\n","        self.dense_3 = Dense(64, activation='sigmoid')\n","        self.dropout_3 = Dropout(0.2)\n","        # self.dense_4 = Dense(64, activation='sigmoid')  # Changed the name to dense_4\n","        # self.dropout_4 = Dropout(0.5)\n","        self.output_layer = Dense(num_classes, activation='sigmoid')\n","        self.softmax = Softmax()\n","\n","    def call(self, inputs):\n","        x = resize_and_rescale(inputs)\n","        x = self.mobilenet_base(x)\n","        x = self.flatten(x)\n","        x = self.dense_1(x)\n","        x = self.dropout_1(x)\n","        x = self.dense_2(x)\n","        x = self.dropout_2(x)\n","        x = self.dense_3(x)\n","        x = self.dropout_3(x)\n","        # x = self.flatten(x)\n","        # x = self.dense_4(x)\n","        # x = self.dropout_4(x)\n","        x = self.output_layer(x)\n","        return self.softmax(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2043,"status":"ok","timestamp":1706369896138,"user":{"displayName":"Quang Tran","userId":"05975989383480383905"},"user_tz":-420},"id":"J9rlHDUbsWVE","outputId":"9b809317-f8b2-4605-afa2-650f147f71c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"mobile_net_v1_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," mobilenetv2_1.00_128 (Func  (None, 1280)              2257984   \n"," tional)                                                         \n","                                                                 \n"," flatten_5 (Flatten)         multiple                  0         \n","                                                                 \n"," dense_18 (Dense)            multiple                  1311744   \n","                                                                 \n"," dropout_10 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_19 (Dense)            multiple                  262400    \n","                                                                 \n"," dropout_11 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_20 (Dense)            multiple                  16448     \n","                                                                 \n"," dropout_12 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_21 (Dense)            multiple                  34450     \n","                                                                 \n"," softmax_5 (Softmax)         multiple                  0         \n","                                                                 \n","=================================================================\n","Total params: 3883026 (14.81 MB)\n","Trainable params: 3848914 (14.68 MB)\n","Non-trainable params: 34112 (133.25 KB)\n","_________________________________________________________________\n"]}],"source":["model = MobileNetV1()\n","model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","model.build(input_shape = (None,128,128,3))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18r2zEtuTzTT"},"outputs":[],"source":["history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=100,\n","    callbacks=[\n","        create_tensorboard_callback(\"training_logs\",\n","                                    \"images_classification\"),\n","        checkpoint_callback,early_stopping\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28riOlNx10_Y"},"outputs":[],"source":["# sample_input, sample_target = next(iter(train_generator))\n","# sample_output = model.predict(sample_input)\n","# print(\"Sample Input Shape:\", sample_input.shape)\n","# print(\"Sample Target Shape:\", sample_target.shape)\n","# print(\"Sample Output Shape:\", sample_output.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"7coZk_3syw1i"},"source":["#MobileNetV2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGo5WYe1vS3z"},"outputs":[],"source":["# import tensorflow as tf\n","# from tensorflow.keras import layers, models\n","\n","# def bottleneck_block(x, filters, expansion, stride,input_shape):\n","#     # Expansion\n","#     x = layers.Conv2D(filters * expansion, 1, padding='same')(x)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.ReLU(6.0)(x)\n","\n","#     # Depthwise convolution\n","#     x = layers.DepthwiseConv2D(3, strides=stride, padding='same')(x)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.ReLU(6.0)(x)\n","\n","#     # Projection\n","#     x = layers.Conv2D(filters, 1, padding='same')(x)\n","#     x = layers.BatchNormalization()(x)\n","\n","#     # Residual connection\n","#     if stride == 1 and x.shape[-1] == input_shape[-1]:\n","#         x = tf.keras.layers.Add()([x, input_tensor])\n","\n","#     return x\n","\n","\n","# # Input layer\n","# input_tensor = layers.Input(shape=(128, 128, 3))\n","\n","# # Initial\n","# x = layers.Conv2D(32, kernel_size= 3, strides=2, padding='same')(input_tensor)\n","# # x = layers.BatchNormalization()(x)\n","# # x = layers.ReLU(6.0)(x)\n","\n","\n","# # Define inverted residual settings\n","# Hyperparameter = [\n","#     (16, 1, 1),\n","#     (24, 6, 2),\n","#     (32, 6, 2),\n","#     (64, 6, 2),\n","#     (96, 6, 1),\n","#     (160, 6, 2),\n","#     (320, 6, 1)\n","# ]\n","\n","# # get inverted residual blocks\n","# for filters, expansion, stride in Hyperparameter:\n","# # Define these settings based on your architecture\n","#     x = bottleneck_block(x, filters, expansion, stride, input_tensor.shape)\n","\n","# # Final layers\n","# x = layers.Conv2D(1280, kernel_size=1, strides=1, padding='same')(x)\n","# x = layers.GlobalAveragePooling2D()(x)\n","# x = layers.Flatten()(x)\n","# # Adjust output neurons for your task\n","# x = layers.Dense(530, activation='sigmoid')(x)\n","# x = layers.Softmax()(x)\n","# model = models.Model(inputs=input_tensor, outputs=x)\n","# model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5z1NZj6ulj4m"},"outputs":[],"source":["history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=1,\n","    # callbacks=[\n","    #     create_tensorboard_callback(\"training_logs\",\n","    #                                 \"images_classification\"),\n","    #     checkpoint_callback,early_stopping\n","    # ]\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}